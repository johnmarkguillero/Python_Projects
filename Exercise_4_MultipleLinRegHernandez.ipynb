{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7D6uqBg1uZI"
      },
      "source": [
        "# Topic : Neural Network for Multiple Linear Regression  \n",
        "## Objective for this template:\n",
        "\n",
        "1. Introduce participants to fundamental concepts of multiple linear regression\n",
        "2. Use tensorflow to build a simple sequential neural network regression model that accepts multiple features.\n",
        "3. Demonstrate the process of inspecting attribute relationships as well as training  and evaluating the performance of the model\n",
        "4. Allow participants to practice adjusting various parameters of the model to improve performance.\n",
        "\n",
        "Designed By: _Rodolfo C. Raga Jr._  __Copyright @2019__\n",
        "\n",
        "__Permission granted to use template for educational purposes so long as this heading is not removed.__\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTOVZNMD1ci2"
      },
      "source": [
        "**Step 1**\n",
        ":Import TensorFlow library as tf.\n",
        "\n",
        "Import keras module from TensorFlow.\n",
        "\n",
        "Import layers module from TensorFlow's Keras API.\n",
        "\n",
        "Import MinMaxScaler class from scikit-learn.\n",
        "\n",
        "Import train_test_split function from scikit-learn.\n",
        "\n",
        "Import r2_score function from scikit-learn.\n",
        "\n",
        "Import pyplot module from matplotlib.\n",
        "\n",
        "Import pandas library as pd.\n",
        "\n",
        "Import seaborn library as sns.\n",
        "\n",
        "Import io module.\n",
        "\n",
        "Print the current version of TensorFlow.\n",
        "\n",
        "Print a message asking the user to select a dataset to load.\n",
        "\n",
        "Import files module from google.colab.\n",
        "\n",
        "Use files.upload() to prompt the user to upload a dataset and store it in the uploaded variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tzXnSzxapxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b04726a5-6b6e-4965-b7c6-0a5bbb81e58f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import io\n",
        "\n",
        "print(\"Done with library declaration. Current version of Tensorflow is :\",tf.__version__)\n",
        "print(\"Select dataset to load...\")\n",
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done with library declaration. Current version of Tensorflow is : 2.13.0\n",
            "Select dataset to load...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-76246f7e-6af9-4ddd-a2bd-4d17579b74f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-76246f7e-6af9-4ddd-a2bd-4d17579b74f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex8CxTMx1cjB"
      },
      "source": [
        "**Step 2** :\n",
        "dataset_raw = pd.read_csv(io.BytesIO(uploaded['auto-mpg.csv'])): This line reads data from the 'auto-mpg.csv' file, which was previously uploaded, and loads it into a Pandas DataFrame. It uses the Pandas library (imported as pd) and the io module to accomplish this.\n",
        "\n",
        "print(\"Done with loading data to dataframes...\"): This line prints a message to indicate that the data has been successfully loaded into DataFrames. It serves as a progress or status update.\n",
        "\n",
        "dataset_raw.head(): This code displays the first few rows of the dataset_raw DataFrame. It provides a quick preview of the loaded data to ensure it was read correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ_IEhmdazze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "2eede187-1149-45fa-d497-65ccf8062045"
      },
      "source": [
        "dataset_raw = pd.read_csv(io.BytesIO(uploaded['auto-mpg.csv']))\n",
        "print(\"Done with loading data to dataframes...\")\n",
        "\n",
        "dataset_raw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-29bac208b2d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auto-mpg.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done with loading data to dataframes...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'auto-mpg.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuC452ds8QHs"
      },
      "source": [
        "**Step 3** :\n",
        "dataset_raw.pop(car name):\n",
        "This line removes the column named \"car name\" from the dataset_raw DataFrame.\n",
        "\n",
        "origin = dataset_raw.pop(origin):\n",
        "It removes the column named \"origin\" from the dataset_raw DataFrame and assigns its values to the variable 'origin.'\n",
        "\n",
        "dataset_raw['USA'] = (origin == 1)*1.0:\n",
        "This line creates a new column 'USA' in the dataset_raw DataFrame. It assigns a value of 1.0 to each row where the 'origin' column has a value of 1 (indicating cars from the USA) and 0.0 to all other rows.\n",
        "\n",
        "dataset_raw['Europe'] = (origin == 2)*1.0:\n",
        "Similar to the previous line, this one creates a new column 'Europe' in the dataset_raw DataFrame. It assigns a value of 1.0 to each row where the 'origin' column has a value of 2 (indicating cars from Europe) and 0.0 to all other rows.\n",
        "\n",
        "dataset_raw['Japan'] = (origin == 3)*1.0:\n",
        "This line creates a new column 'Japan' in the dataset_raw DataFrame. It assigns a value of 1.0 to each row where the 'origin' column has a value of 3 (indicating cars from Japan) and 0.0 to all other rows.\n",
        "\n",
        "dataset_raw.head():\n",
        "this code displays the first few rows of the modified dataset_raw DataFrame, which now includes the 'USA,' 'Europe,' and 'Japan' columns based on the 'origin' values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxuvwChB-Bw-"
      },
      "source": [
        "dataset_raw.pop(\"car name\")\n",
        "#dataset_raw.isna().sum()\n",
        "#dataset_raw = dataset_raw.dropna()\n",
        "origin = dataset_raw.pop('origin')\n",
        "dataset_raw['USA'] = (origin == 1)*1.0\n",
        "dataset_raw['Europe'] = (origin == 2)*1.0\n",
        "dataset_raw['Japan'] = (origin == 3)*1.0\n",
        "dataset_raw.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVC-gHcl0Hna"
      },
      "source": [
        "**Step 4** :\n",
        "sns.pairplot(dataset_raw[[\"mpg\", \"cylinders\", \"displacement\", \"weight\", \"acceleration\"]], diag_kind=\"kde\"):\n",
        "This line uses Seaborn (imported as sns) to create a pairplot. A pairplot is a grid of scatterplots showing relationships between pairs of variables. In this case, it's plotting the variables \"mpg,\" \"cylinders,\" \"displacement,\" \"weight,\" and \"acceleration\" from the dataset_raw DataFrame. The `diag_kind=\"kde\"` argument specifies that the diagonal plots should be kernel density estimate (KDE) plots instead of histograms.\n",
        "\n",
        "train_stats = dataset_raw.describe():\n",
        "This line computes summary statistics for the dataset_raw DataFrame using the describe() method. It calculates statistics like count, mean, standard deviation, minimum, and maximum for each numerical column in the DataFrame. The results are stored in the train_stats variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jySLerN4aKKB"
      },
      "source": [
        "sns.pairplot(dataset_raw[[\"mpg\", \"cylinders\", \"displacement\", \"weight\", \"acceleration\"]], diag_kind=\"kde\")\n",
        "\n",
        "train_stats = dataset_raw.describe()\n",
        "#train_stats.pop(\"mpg\")\n",
        "#train_stats = train_stats.transpose()\n",
        "#train_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLfaXjMObzGK"
      },
      "source": [
        "**Step 5** :\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)):\n",
        "This line creates an instance of the MinMaxScaler class (imported from scikit-learn). The `feature_range=(0, 1)` argument specifies that the scaler should scale features to a range between 0 and 1.\n",
        "\n",
        "rescaledDS = scaler.fit_transform(dataset_raw):\n",
        "It applies the MinMax scaling transformation to the dataset_raw DataFrame using the scaler instance created earlier. This scales all numerical features in the DataFrame to the specified feature range (0 to 1) and stores the scaled values in the rescaledDS variable.\n",
        "\n",
        "rescaledDF = pd.DataFrame(rescaledDS):\n",
        "This line creates a new Pandas DataFrame, rescaledDF, from the rescaledDS NumPy array. It's done to maintain the scaled data in a DataFrame format.\n",
        "\n",
        "print(rescaledDF.head()): this code prints the first few rows of the rescaledDF DataFrame, showing the scaled values of the dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ8Wv4-IoEBK"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaledDS = scaler.fit_transform(dataset_raw)\n",
        "rescaledDF = pd.DataFrame(rescaledDS)\n",
        "print(rescaledDF.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUkDy0Uybquf"
      },
      "source": [
        "**Step 6** :\n",
        "predictor_dataset = rescaledDF.iloc[:, 1:10]:\n",
        "This line creates a new DataFrame, predictor_dataset, by selecting all rows and columns from the rescaledDF DataFrame, but only columns from index 1 to 9 (10th column is excluded). These columns are likely the predictor variables for a machine learning model.\n",
        "\n",
        "target_dataset = rescaledDF.iloc[:0]:\n",
        "There is an error in this line. It attempts to create a target_dataset DataFrame by selecting rows up to index 0 (which would be an empty DataFrame) from rescaledDF. This line should be corrected to select the appropriate target variable(s) from the DataFrame.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictor_dataset, target_dataset, random_state=42, test_size=0.3):\n",
        "This line uses the train_test_split function from scikit-learn to split the predictor_dataset and target_dataset into training and testing subsets. The random_state parameter is set to 42 to ensure reproducibility, and the test_size parameter specifies that 30% of the data should be used for testing. The resulting datasets are assigned to X_train, X_test, y_train, and y_test variables.\n",
        "\n",
        "print(\"Done with data separation...\"):\n",
        "This line prints a message to indicate that the data separation into training and testing sets has been completed.\n",
        "\n",
        "print(y_train.tail()):\n",
        "This code prints the last few rows of the y_train dataset. It's a useful step to inspect a part of the target variable for the training set.\n",
        "\n",
        "X_train.tail():\n",
        "This would have printed the last few rows of the X_train dataset, which contains the predictor variables for the training set. However, there's a typo in this line; it should be corrected to X_train.tail() to execute properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN0-7f4Ubpbe"
      },
      "source": [
        "predictor_dataset = rescaledDF.iloc[:,1:10]\n",
        "target_dataset = rescaledDF.iloc[:0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictor_dataset,target_dataset, random_state=42, test_size=0.3)\n",
        "print(\"Done with data separation...\")\n",
        "\n",
        "print(y_train.tail())\n",
        "X_train.tail()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DINSOKUp-N5g"
      },
      "source": [
        "**Step 7**:\n",
        "This line of code initializes a new neural network model using TensorFlow and Keras. The tf.keras.Sequential() function creates an empty sequential model, which is a linear stack of layers. In this model, you can add layers one after the other to build a neural network architecture. The sequential model is suitable for building feedforward neural networks where data flows sequentially from one layer to the next. You can add layers to this model using the .add() method, specifying the type and configuration of each layer as you build your neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-R5IvgadXqC"
      },
      "source": [
        "model =  tf.keras.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp9vDht3CKFc"
      },
      "source": [
        "**Step 8**:\n",
        "\n",
        "Set 1 (with activation functions):\n",
        "1. layer_0 = tf.keras.layers.Dense(units=40, activation=relu, input_shape=[len(X_train.keys())]): This line defines the first layer (input layer) with 40 units (neurons), ReLU (Rectified Linear Unit) activation function, and an input shape based on the number of features in the training data.\n",
        "\n",
        "2. layer_1 = tf.keras.layers.Dense(units=40, activation=relu): This line defines the second hidden layer with 40 units and a ReLU activation function.\n",
        "\n",
        "3. layer_2 = tf.keras.layers.Dense(units=10, activation=relu): This line defines the third hidden layer with 10 units and a ReLU activation function.\n",
        "\n",
        "4. layer_3 = tf.keras.layers.Dense(units=1): This line defines the output layer with 1 unit (typically used for regression tasks), and no activation function (i.e., it uses a linear activation by default).\n",
        "\n",
        "Set 2 (without activation functions):\n",
        "the layers are defined similarly, but without specifying activation functions. In this case, linear activation functions are used by default for all layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIRk26hmepaY"
      },
      "source": [
        "layer_0 = tf.keras.layers.Dense (units=40, activation=\"relu\", input_shape=[len(X_train.keys())])\n",
        "layer_1 = tf.keras.layers.Dense (units=40, activation=\"relu\",)\n",
        "layer_2 = tf.keras.layers.Dense (units=10, activation=\"relu\",)\n",
        "layer_3 = tf.keras.layers.Dense(units=1)\n",
        "\n",
        "#layer_0 = tf.keras.layers.Dense (units=40, input_shape=[len(X_train.keys())])\n",
        "#layer_1 = tf.keras.layers.Dense (units=40)\n",
        "#layer_2 = tf.keras.layers.Dense (units=10)\n",
        "#layer_3 = tf.keras.layers.Dense(units=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4WkggcfFzaC"
      },
      "source": [
        "**Step 9**:\n",
        "#model = tf.keras.Sequential([layer_0, layer_1, layer_2]):\n",
        "It appears to be an attempt to create a sequential model (a neural network) and add layers layer_0, layer_1, and layer_2 to it in a single step.\n",
        "\n",
        "model.add(layer_0):\n",
        "This line adds the layer_0 (input layer) to the model. It's an example of how to add a layer to a neural network model sequentially.\n",
        "\n",
        "model.add(layer_1):\n",
        "This line adds the layer_1 (hidden layer) to the model. It's an example of how to add another layer to the neural network model.\n",
        "\n",
        "model.add(layer_2):\n",
        "This line adds the layer_2 (another hidden layer) to the model. It's an example of how to add yet another layer to the neural network model.\n",
        "\n",
        "model.add(layer_3):\n",
        "This line adds the layer_3 (output layer) to the model. It's an example of how to add the output layer to the neural network model.\n",
        "\n",
        "model.summary():\n",
        "This code prints a summary of the neural network model, including information about the layers, the number of parameters in each layer, and the total number of parameters in the model. It's a useful way to inspect the architecture and size of your neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuqUagrie1P3"
      },
      "source": [
        "#model = tf.keras.Sequential([layer_0,layer_1,layer_2])\n",
        "model.add(layer_0)\n",
        "model.add(layer_1)\n",
        "model.add(layer_2)\n",
        "model.add(layer_3)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ38HWvOHX9S"
      },
      "source": [
        "**Step 10**:\n",
        "The provided code compiles a neural network model with the following configuration:\n",
        "\n",
        "- Loss function: Mean squared error (MSE), commonly used for regression tasks.\n",
        "- Optimizer: Adam optimizer, a popular choice for training neural networks.\n",
        "- Evaluation metrics: Mean absolute error (MAE) and mean squared error (MSE) for assessing model performance.\n",
        "\n",
        "After compiling the model, it prints a message indicating that the compilation is complete. Compiling a model is a crucial step in setting up its training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ39qp4Be7xn"
      },
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "print(\"Done with compile\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-IYiDxhHkjh"
      },
      "source": [
        "**Step 11**:\n",
        "The provided code trains a neural network model with the following configuration:\n",
        "\n",
        "- Training data: It uses `X_train` as the input features and `y_train` as the target values.\n",
        "- Number of epochs: The model is trained for 100 epochs.\n",
        "- Verbosity: During training, it displays progress information for each epoch.\n",
        "- After training, it prints a message indicating the completion of model training and displays a summary of the neural network model, including its architecture and the number of parameters in each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iYGJMLVfQ2G"
      },
      "source": [
        "trained_model = model.fit(X_train, y_train, epochs=100, verbose=1)\n",
        "print(\"Done with model training\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTT_jCW5H2kQ"
      },
      "source": [
        "**Step 12**:\n",
        "Certainly, here's the summary without any quotes:\n",
        "\n",
        "The provided code:\n",
        "\n",
        " Uses a trained neural network model to make predictions (y_pred) on a test dataset (X_test).\n",
        "\n",
        " Prints the sizes (number of elements) of the predicted values (y_pred) and the actual target values (y_test).\n",
        "\n",
        " Prints the actual target values (y_test) under the header \"Actual Values.\"\n",
        "\n",
        " Prints the predicted values (y_pred) under the header \"Predicted Values.\" The predicted values are reshaped to be displayed in a single row.\n",
        "\n",
        " Calculates the R-squared (R2) score to assess the goodness of fit of the model's predictions compared to the actual target values in the test dataset.\n",
        "\n",
        " Prints the overall R2 score as a percentage to evaluate the model's performance in predicting the target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0xuTOyzIBjJ"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(y_pred.size)\n",
        "print(y_test.size)\n",
        "print('Actual Values')\n",
        "print(y_test)\n",
        "\n",
        "print('Predicted Values')\n",
        "print(y_pred.reshape(1,-1))\n",
        "\n",
        "score=r2_score (y_test,y_pred)\n",
        "print(\"Overall score: {}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCyq_uHyIINi"
      },
      "source": [
        "Other things we can do:\n",
        "1. Analyze training statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-niV7nX3f2AC"
      },
      "source": [
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel(\"Loss Magnitude\")\n",
        "plt.plot(trained_model.history['loss'])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}